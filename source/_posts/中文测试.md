---
title: 完全無關，日後補助要從哪裡來？
date: 2020-02-19 23:23
tags: [Chinese, Placeholder, Test]
---

## 完全無關

看似完美，看似完美，我是網頁設計課的同學，架構了一個網站寫好原始碼之後過來打分數，同學一整學期沒有上過任何課，同學一整學期沒有上過任何課，甚至他沒有出席過任何一堂課、甚至他沒有出席過任何一堂課，甚至他沒有出席過任何一堂課、甚至他沒有出席過任何一堂課，呵呵、呵呵，在學期末之後，…現在我不敢肯定，那麼餘下四分之三的時光請讓我來守護你好嗎？不然不曉得奮鬥，你們快點出名吧，生，一旦累死了，在於蠢的無怨無悔，可是已經晚了……懷才就像懷孕，.早起的鳥兒有蟲吃，生，女孩富著養，你們快點出名吧，男人之美，如果多吃魚可以補腦讓人變聰明的話，工作，問題是沒錢！年登，趙啟正：深化改革的時機到了，一人分飾男女二角，無照騎機車出遊，烏坎村選舉落幕，、5D，《傳產》華航攜手淡大，怕曬愛喝咖啡，瑤瑤想一槍打死 Apple 吃了郭書瑤，小戴超威，全球第 2 人治好愛滋病-，大馬網球公開賽，送樹苗造樹海，朴敏英化身短髮俏護士，政期剩兩年馬英九該如何翻身？我們見小孩子在草裡在沙堆裡在淺水裡打滾作樂，為什麼要到這時候，各各不同，只許你獨身；因為有了伴多少總得叫你分心，她們也使我，許是恨，平常我們從自己家裡走到朋友的家裡，但那晚雖則結識了一個可愛的小友，同是一滴眼淚，你得有力量翻起那岩石才能把它不傷損的連根起出誰知道那根長的多深！馬賽沒有傳說中大街小巷塞滿罪犯般的恐怖，我刚想起来，我最愛的鴨頸，還是雀巢性價比最高，肉燥麵？

## 背景

最近的一个项目中需要用 PyTorch 实现一套可微分的 Non-local Means (NLM) 降噪算法，并通过样本学习建立一个图像内容与降噪强度参数 的映射模型。

在 PyTorch 框架下，实现传统图像处理算法往往有着不止一种的写法，例如，为了实现 的均值滤波，我们既可以使用一个归一化的 box filter 对图像进行卷积，也可以对图像进行 9 次位移（shift/roll）并逐像素求和，最后再将每个像素值除以 9。在模型进行正向推断（forward）时，不同的写法在显存开销上或许不存在太大的区别，但是当我们使用 Autograd 进行逐层的梯度反向传播（backward）时，不同的写法往往对应了截然不同的显存占用。以 NLM 为例，在我的实验中，对于同一张 的输入图像，不同的 tensor 操作写法可以带来 180%以上的训练显存差异。当然，在代码设计没有明显缺陷的情况下，节省显存必然意味着运算效率的下降，不过对于我的这个项目来说，训练阶段的耗时并不是瓶颈所在，batch size 才是影响模型性能的主要因素，因此利用时间换取空间仍然是一个非常划算的选择。

先简单地回顾一下 Non-local Means 算法：给定一幅噪声图像，对于图像中的每个像素 ，以其为中心，在一个 的搜索窗口（下文代码中以 search_window 命名）内，遍历所有的可能的图像块（一般是一个 的方形区域，且有 ，下文代码中以 patch 命名），并计算该图像块与以 为中心的那个图像块之间的相似度。

在 的搜索窗口内，一共存在
个可重叠的图像块，对于第 个图像块，假设其与 所在图像块之间的相似度为
（注意：是图像块之间的相似度，而不是像素与像素之间的相似度！），且该图像块中心像素的像素值为
，那么对于 像素，其经过 NLM 降噪后的像素值可以表示为：
